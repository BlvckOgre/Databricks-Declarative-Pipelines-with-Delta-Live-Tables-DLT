# ğŸ“Š Databricks Declarative Pipelines with Delta Live Tables (DLT)

This project demonstrates a **production-grade data pipeline** built using **Databricks Delta Live Tables (DLT)** â€” a fully managed, declarative ETL framework for building reliable, maintainable data pipelines at scale.

## ğŸš€ Key Features

- ğŸ” **Delta Live Tables (DLT)**: Declarative pipeline using SQL and PySpark for automatic lineage, orchestration, and quality checks.
- ğŸ”„ **Slowly Changing Dimensions (SCD)**:
  - **Type 1** â€“ Overwrites old data with new values for changed records.
  - **Type 2** â€“ Preserves historical data by tracking changes over time using surrogate keys and timestamps.
- ğŸ“ˆ **Monitoring & Observability**:
  - Built-in data quality checks with DLT expectations.
  - DLT UI for pipeline status, event logs, and lineage visualization.
- ğŸš¨ **Alerts & Notifications**:
  - Integration with Databricks Jobs and alerting tools for pipeline failures or data quality violations.

## ğŸ—ï¸ Architecture Overview

