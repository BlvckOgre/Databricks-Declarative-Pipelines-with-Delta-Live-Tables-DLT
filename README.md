# ğŸ” StreamDLT: A Declarative CDC Pipeline with Delta Live Tables

## ğŸš€ Overview

**StreamDLT** is a modular and scalable end-to-end data pipeline built using **Databricks Delta Live Tables (DLT)**, designed to simulate a real-world retail environment with multiple branches ("East" and "West") and a product/customer master data system.

The project uses the **Medallion Architecture (Bronze â†’ Silver â†’ Gold)** and demonstrates how to implement **Streaming Ingestion**, **Change Data Capture (CDC)**, and **Slowly Changing Dimensions (SCD Type 1 and Type 2)** declaratively.

---

## ğŸ“ Project Structure

DLT_Pipeline/
â”œâ”€â”€ source_code_transformations/
â”‚ â”œâ”€â”€ bronze/
â”‚ â”‚ â”œâ”€â”€ sales_ingestion.py
â”‚ â”‚ â”œâ”€â”€ products_ingestion.py
â”‚ â”‚ â””â”€â”€ customers_ingestion.py
â”‚ â”œâ”€â”€ silver/
â”‚ â”‚ â”œâ”€â”€ sales_enr.py
â”‚ â”‚ â”œâ”€â”€ transformed_products.py
â”‚ â”‚ â””â”€â”€ transformed_customers.py
â”‚ â””â”€â”€ gold/
â”‚ â”œâ”€â”€ dim_products.py
â”‚ â”œâ”€â”€ dim_customers.py
â”‚ â”œâ”€â”€ fact_sales.py
â”‚ â””â”€â”€ business_sales.py
â”œâ”€â”€ DWH_source.sql
â””â”€â”€ README.md

---

## ğŸ§± Architecture

---


---

## ğŸ› ï¸ Technologies

- **Databricks Delta Live Tables (DLT)**
- **Spark Structured Streaming**
- **PySpark**
- **Delta Lake (ACID Tables)**
- **Medallion Architecture**
- **SCD Type 1 and Type 2**
- **Materialized Views**
- **Streaming Views**
- **Data Quality Expectations**

---

## ğŸ”„ Bronze Layer: Ingestion & Quality Rules

### âœ… Ingested Streaming Tables:
- `sales_stg`: Unified sales data from `sales_east` and `sales_west`.
- `products_stg`, `customers_stg`: Master data from respective source tables.

### âœ… Data Quality with Expectations:
- Applied `@dlt.expect_all_or_drop` for basic checks:
  - `product_id IS NOT NULL`, `price >= 0`
  - `customer_id IS NOT NULL`, `customer_name IS NOT NULL`

---

## ğŸ§ª Silver Layer: Transformations & CDC (Type 1)

### âœ… Key Logic:
- Created **views** (`*_enr_view`) with added business logic (e.g., calculated columns, name formatting).
- Used `dlt.create_auto_cdc_flow()` for **SCD Type 1** to handle changes without history.

### â—Note:
- Streaming **views** are used to feed downstream Gold layers to avoid issues with upserted streaming tables.

---

## ğŸª™ Gold Layer: Dimensional Modeling & Aggregates

### ğŸ“¦ `dim_products`, `dim_customers`
- Built with **SCD Type 2** logic to preserve history.
- Used `stored_as_scd_type = 2` in `create_auto_cdc_flow()`.

### ğŸ“Š `fact_sales`
- Upserted with **SCD Type 1** (no history).
- Designed to align with best practices (no SCD 2 on fact tables).

### ğŸ“ˆ `business_sales1`
- A **materialized view** joining fact and dimension tables.
- Aggregates total sales by `region` and `category`.
- Guarantees accurate results across all historical data.

---

## ğŸ” CDC Simulation: SQL Inserts

Data changes are simulated in `DWH_source.sql`:
- New sales records (incremental inserts)
- Product price & name changes
- Customer region & name corrections

These updates trigger auto-upserts and history tracking in the pipeline.

---

## âœ… Key Takeaways

- ğŸ“Œ **DLT enables simplified orchestration** without external schedulers.
- ğŸ›¡ï¸ **Expectations enforce data quality** early in the pipeline.
- ğŸ” **CDC + SCD logic** supports realistic business scenarios (corrections, updates).
- âš™ï¸ **Streaming views** solve challenges with upserted streaming tables.
- ğŸ“Š **Materialized views** ensure consistent business aggregations.

---

## ğŸš¦ How to Use / Run

1. Load the `DWH_source.sql` data into your Databricks SQL environment.
2. Deploy the pipeline by referencing `source_code_transformations/` in DLT.
3. Run a dry test to validate streaming append logic.
4. Validate CDC handling with incremental inserts (see DWH_source.sql).
5. Visualize results via dashboards or queries on `business_sales1`.

---

## Monitoring

In the jobs and pipeline area of databricks under development tab of your pipeline you can monitor your pipeline.
- configure alerts
- schedule the pipeline
- monitor runs

---

## ğŸ“Œ Final Notes

This project demonstrates how to simulate a modern retail data pipeline using declarative constructs, without writing complex orchestration code. The modular design makes it easy to extend with more branches, business domains, or additional validations.

---

## ğŸ™Œ Acknowledgements

Built using:
- Databricks Delta Live Tables
- Apache Spark
- PySpark and SQL
- Realistic retail business logic

---

## ğŸ§  Learn More

- [Delta Live Tables Documentation](https://docs.databricks.com/data-engineering/delta-live-tables/index.html)
- [SCD Type 1 vs Type 2](https://www.datawarehouse4u.info/SCD-Slowly-Changing-Dimension.html)
