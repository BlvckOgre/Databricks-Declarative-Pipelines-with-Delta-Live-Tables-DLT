# ğŸ“Š Databricks Declarative Pipelines with Delta Live Tables (DLT)

This project demonstrates a **production-grade data pipeline** built using **Databricks Delta Live Tables (DLT)** â€” a fully managed, declarative ETL framework for building reliable, maintainable data pipelines at scale.

## ğŸš€ Key Features

- ğŸ” **Delta Live Tables (DLT)**: Declarative pipeline using SQL and PySpark for automatic lineage, orchestration, and quality checks.
- ğŸ”„ **Slowly Changing Dimensions (SCD)**:
  - **Type 1** â€“ Overwrites old data with new values for changed records.
  - **Type 2** â€“ Preserves historical data by tracking changes over time using surrogate keys and timestamps.
- ğŸ“ˆ **Monitoring & Observability**:
  - Built-in data quality checks with DLT expectations.
  - DLT UI for pipeline status, event logs, and lineage visualization.
- ğŸš¨ **Alerts & Notifications**:
  - Integration with Databricks Jobs and alerting tools for pipeline failures or data quality violations.

## ğŸ—ï¸ Architecture Overview


        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Raw Sources â”‚
        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
              â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Bronze DLT Table   â”‚  â† Raw ingestion
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Silver DLT Table   â”‚  â† SCD logic applied
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Gold DLT Table     â”‚  â† Analytics-ready
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


## ğŸ“‚ Project Structure

â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ 01_bronze_dlt.py
â”‚ â”œâ”€â”€ 02_silver_scd_type1.py
â”‚ â”œâ”€â”€ 03_silver_scd_type2.py
â”‚ â””â”€â”€ 04_gold_tables.py
â”œâ”€â”€ config/
â”‚ â””â”€â”€ expectations.json # Data quality rules
â”œâ”€â”€ alerts/
â”‚ â””â”€â”€ dlt_failure_alerts.json # Notification configs
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt



## âš™ï¸ Technologies Used

- ğŸ§± **Databricks**
- âš¡ **Delta Live Tables**
- ğŸ **PySpark / Spark SQL**
- ğŸ“œ **Delta Lake**
- ğŸ“¬ **Webhooks / Alerting Tools**
- ğŸ“Š **Unity Catalog** (optional)

## ğŸ§ª Data Quality & Expectations

DLT Expectations are defined to:

- Ensure primary key uniqueness
- Detect nulls in critical columns
- Validate schema and value constraints

Failing expectations can either drop bad records or halt the pipeline based on severity.

## ğŸ”” Alerts & Monitoring

- Integrated alerting on DLT pipeline failure or anomalies
- Logs available via the DLT UI and Databricks workspace
- Optionally send notifications to Slack, Teams, or Email using webhooks

## ğŸ“ˆ Use Cases

This project is ideal for:

- Implementing change data capture (CDC) logic in batch/streaming pipelines
- Building Type 1 and Type 2 dimension tables
- Observing and alerting on production data quality issues

## ğŸ“ Future Enhancements

- Add unit tests with `pytest`
- CI/CD deployment via Databricks CLI or Terraform
- Integration with Unity Catalog for fine-grained access control

## ğŸ§  Learn More

- [Delta Live Tables Documentation](https://docs.databricks.com/data-engineering/delta-live-tables/index.html)
- [SCD Type 1 vs Type 2](https://www.datawarehouse4u.info/SCD-Slowly-Changing-Dimension.html)
